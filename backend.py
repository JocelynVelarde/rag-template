import streamlit as st
import hashlib
from pymongo import MongoClient
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_mongodb import MongoDBAtlasVectorSearch
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from langchain_core.runnables import RunnablePassthrough
from pypdf import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter 
import time 

MONGO_URI = st.secrets["MONGO_URI"]
DB_NAME = "vector_storedb"
COLLECTION_NAME = "embeddings_rag"
ATLAS_VECTOR_SEARCH = "vector_index_rag"

def get_vector_store():
    """
    Returns a MongoDB Atlas Vector Search object that can be used to search for embeddings
    in a MongoDB collection.

    Parameters
    ----------
    None

    Returns
    -------
    MongoDBAtlasVectorSearch
        A MongoDB Atlas Vector Search object that can be used to search for embeddings in a MongoDB collection.
    """
    client = MongoClient(MONGO_URI)
    collection = client[DB_NAME][COLLECTION_NAME] # to connect to mongo collection

    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

    vector_store = MongoDBAtlasVectorSearch(
        collection=collection,
        embedding=embeddings,
        index_name=ATLAS_VECTOR_SEARCH,
        # text_key="text"
    )
    return vector_store


# covnert text to embeddings 
def ingest_text(texts):
    """
    Ingests a large body of text into a MongoDB Atlas Vector Search collection.

    The text is split into chunks of 500 characters with a 100 character overlap, and each chunk is embedded using the Google Generative AI model. The embeddings are then added to the MongoDB Atlas Vector Search collection in batches of 5 chunks at a time, with a 2 second delay between batches to respect rate limits.

    Parameters
    ----------
    texts : str
        The text to ingest.

    Returns
    -------
    bool
        True if the text was ingested successfully, False otherwise.
    """
    vector_store = get_vector_store()
    text_hash = hashlib.md5(texts.encode()).hexdigest()
    
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=100
    )
    #input to db 
    chunks = splitter.create_documents([texts], metadatas=[{"hash": text_hash}])
    
   
    batch_size = 5  # Process 5 chunks at a time
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i + batch_size]
        print(f"Adding batch {i//batch_size + 1}...")
        vector_store.add_documents(batch) # docs added to vector store
        time.sleep(2) # Wait 2 seconds between batches to respect rate limits
    
    return True





def get_rag_response(query: str) -> str:
    """
    Generate a response to a query using a RAG system.

    The system consists of a retriever that fetches relevant text snippets from a database,
    a prompt template that combines the query and the retrieved text snippets, and a language model
    that generates a response based on the prompt.

    Parameters
    ----------
    query : str
        The query to generate a response to.

    Returns
    -------
    str
        The response generated by the RAG system.
    """
    vector_store = get_vector_store()

    retriever = vector_store.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 3}
    )

    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.2
    )

    prompt = PromptTemplate(
        template="""
You are a helpful assistant.
Use the following context to answer the question accurately.
If the answer is not in the context, say you don't know.

Context:
{context}

Question:
{question}

Answer:
""",
        input_variables=["context", "question"]
    )

    chain = (
        {
            "context": retriever,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
    )

    response = chain.invoke(query)
    return response.content

def extract_text_from_pdf(pdf_file):
    """
    Extracts text from a PDF file.

    Parameters
    ----------
    pdf_file : str
        The path to the PDF file to extract text from.

    Returns
    -------
    str
        The extracted text.
    """
    text = ""
    pdf_reader = PdfReader(pdf_file)
    for page in pdf_reader.pages:
        text += page.extract_text()
    return text
